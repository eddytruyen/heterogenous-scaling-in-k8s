spark-bench = {
  spark-submit-parallel = false
  spark-submit-config = [{
    spark-args = {
      master = "spark://Release-master-svc.svc.cluster.local:7077"
      //Release-master-svc.Namespace.svc.cluster.local:7077"
      executor-memory = "13G"
      //total-executor-cores = "26"
      //executor-cores = "2"
      driver-memory = "14G"
      //deploy-mode = "cluster"
      //num-executors = 2
    }
    conf = {
     //spark.memory.fraction = 0.75
     spark.memory.storageFraction = 0.90
     //spark.executor.memoryOverhead = "1024"
     spark.driver.memoryOverhead = 1024 
     //spark.sql.shuffle.partitions = 500
     //spark.default.parallelism = 500
      //spark.executor.extraJavaOptions = "-XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap"
      //spark.driver.extraJavaOptions = "-XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap"
      //spark.dynamicAllocation.enabled = "true"
    }
    suites-parallel = true
    workload-suites = [
     {
        descr = "Run two different SQL queries over the dataset in two different formats"
        benchmark-output = "file:///opt/bitnami/spark/spark_data/spark-bench/results-sql-gx-t-y.csv"
        parallel = false
        repeat = 1
        save-mode = "append"
        workloads = [
          {
            name = "sql"
            input = ["file:///opt/bitnami/spark/spark_data/spark-bench-test/kmeans-data-gx-y.csv"]
            query = ["select c0, c6 from input where c0 < -0.9"]
            cache = false
            //partitions = p
            //output = "file:///opt/bitnami/spark/spark_data/spark-bench/output-sql-gx-t-y.parquet"
            save-mode = "overwrite"
          }
        ]

     }
