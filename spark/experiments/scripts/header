spark-bench = {
  spark-submit-parallel = false
  spark-submit-config = [{
    spark-args = {
      master = "spark://172.17.13.105:31934"
      executor-memory = "28G"
      driver-memory = "21G"
      //deploy-mode = "cluster"
      //num-executors = 2
    }
    conf = {
     //spark.memory.fraction = 0.75
     //spark.memory.storageFraction = 0.90
     spark.executor.memoryOverhead = 1000
     spark.driver.memoryOverhead = 3000
     //spark.sql.shuffle.partitions = 500
     //spark.default.parallelism = 500
      //spark.executor.extraJavaOptions = "-XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap"
      //spark.driver.extraJavaOptions = "-XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap"
      // Any configuration you need for your setup goes here, like:
      //spark.dynamicAllocation.enabled = "true"
    }
    suites-parallel = true
    workload-suites = [
      {
        descr = "Run two different SQL queries over the dataset in two different formats"
        benchmark-output = "file:///opt/bitnami/spark/spark_data/spark-bench/results-sql-g2-t-1.csv"
        parallel = false
        repeat = 10
        save-mode = "append"
        workloads = [
          {
            name = "sql"
            input = ["file:///opt/bitnami/spark/spark_data/spark-bench-test/kmeans-data.csv", "file:///opt/bitnami/spark/spark_data/spark-bench-test/kmeans-data-g2-1.parquet"]
            query = ["select * from input", "select c0, c6 from input where c0 < -0.9"]
            cache = false
            partitions = 200
            output = "file:///opt/bitnami/spark/spark_data/spark-bench/output-sql-g2-t-1.parquet"
            save-mode = "overwrite"
          }
        ]

     }
