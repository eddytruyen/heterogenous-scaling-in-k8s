---
# Source: spark/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-spark-secret
  labels:
    app.kubernetes.io/name: spark
    helm.sh/chart: spark-4.1.0
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
---
# Source: spark/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-spark-headless
  labels:
    app.kubernetes.io/name: spark
    helm.sh/chart: spark-4.1.0
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  clusterIP: None
  selector:
    app.kubernetes.io/name: spark
    app.kubernetes.io/instance: my-release
---
# Source: spark/templates/svc-master.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-spark-master-svc
  labels:
    app.kubernetes.io/name: spark
    helm.sh/chart: spark-4.1.0
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
spec:
  type: NodePort
  ports:
    - port: 7077
      targetPort: cluster
      name: cluster
    - port: 80
      targetPort: http
      name: http
      protocol: TCP
  selector:
    app.kubernetes.io/name: spark
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: master
---
# Source: spark/templates/hpa-worker.yaml
apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  labels:
    app.kubernetes.io/name: spark
    helm.sh/chart: spark-4.1.0
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: worker-autoscaler
  name: my-release-spark-autoscaler
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: statefulset
    name: my-release-spark-worker
  minReplicas: 2
  maxReplicas: 5
  metrics:
    - type: Resource
      resource:
        name: cpu
        targetAverageUtilization: 10
---
# Source: spark/templates/statefulset-master.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-spark-master
  labels:
    app.kubernetes.io/name: spark
    helm.sh/chart: spark-4.1.0
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: master
spec:
  serviceName: my-release-spark-headless
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: spark
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: master
  template:
    metadata:
      labels:
        app.kubernetes.io/name: spark
        helm.sh/chart: spark-4.1.0
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: master
    spec:
      
      securityContext:
        fsGroup: 0
        runAsUser: 0
      containers:
        - name: spark-master
          image: docker.io/bitnami/spark:2.4.6-debian-10-r14
          imagePullPolicy: "IfNotPresent"
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
            - name: cluster
              containerPort: 7077
          volumeMounts:
            - name: spark-data
              mountPath: '/opt/bitnami/spark/spark_data'
          env:
            - name: SPARK_MODE
              value: "master"
            - name: SPARK_DAEMON_MEMORY
              value: 
            - name: SPARK_MASTER_PORT
              value: "7077"
            - name: SPARK_MASTER_WEBUI_PORT
              value: "8080"
          livenessProbe:
            httpGet:
              path: /
              port: 8080
            initialDelaySeconds: 180
            periodSeconds: 20
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            httpGet:
              path: /
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          resources:
            limits:
              cpu: 2
              memory: 2Gi
            requests:
              cpu: 1
              memory: 1Gi
      volumes:
  volumeClaimTemplates:
  - metadata:
      name: spark-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "local-storage"
      resources:
        requests:
          storage: 30Gi
---
# Source: spark/templates/statefulset-worker.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-spark-worker
  labels:
    app.kubernetes.io/name: spark
    helm.sh/chart: spark-4.1.0
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: worker
spec:
  serviceName: my-release-spark-headless
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: spark
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: worker
  template:
    metadata:
      labels:
        app.kubernetes.io/name: spark
        helm.sh/chart: spark-4.1.0
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: worker
    spec:
      
      securityContext:
        fsGroup: 0
        runAsUser: 0
      containers:
        - name: spark-worker
          image: docker.io/bitnami/spark:2.4.6-debian-10-r14
          imagePullPolicy: "IfNotPresent"
          ports:
            - name: http
              containerPort: 8081
              protocol: TCP
          volumeMounts:
            - name: spark-data
              mountPath: '/opt/bitnami/spark/spark_data'
          env:
            - name: SPARK_MODE
              value: "worker"
            - name: BASH_DEBUG
              value: "0"
            - name: SPARK_DAEMON_MEMORY
              value: 
            ## There are some environment variables whose existence needs
            ## to be checked because Spark checks if they are null instead of an
            ## empty string
            - name: SPARK_WORKER_WEBUI_PORT
              value: "8081"
            - name: SPARK_DAEMON_JAVA_OPTS
              value: 
            - name: SPARK_MASTER_URL
              value: spark://my-release-spark-master-svc:7077
            # If you use a custom properties file, it must be loaded using a ConfigMap
            - name: SPARK_WORKER_OPTS
              value: 
          livenessProbe:
            httpGet:
              path: /
              port: 8081
            initialDelaySeconds: 180
            periodSeconds: 20
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            httpGet:
              path: /
              port: 8081
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          resources:
            limits:
              cpu: 2
              memory: 2Gi
            requests:
              cpu: 1
              memory: 1Gi
      volumes:
  volumeClaimTemplates:
  - metadata:
      name: spark-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "local-storage"
      resources:
        requests:
          storage: 30Gi
