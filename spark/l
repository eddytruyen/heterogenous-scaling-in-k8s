apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2020-12-14T16:14:55Z"
  generateName: my-release-spark-worker-
  labels:
    app.kubernetes.io/component: worker
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: spark
    controller-revision-hash: my-release-spark-worker-6f9984d8d7
    helm.sh/chart: spark-4.1.0
    statefulset.kubernetes.io/pod-name: my-release-spark-worker-0
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:generateName: {}
        f:labels:
          .: {}
          f:app.kubernetes.io/component: {}
          f:app.kubernetes.io/instance: {}
          f:app.kubernetes.io/managed-by: {}
          f:app.kubernetes.io/name: {}
          f:controller-revision-hash: {}
          f:helm.sh/chart: {}
          f:statefulset.kubernetes.io/pod-name: {}
        f:ownerReferences:
          .: {}
          k:{"uid":"505ce7bc-d2c7-4646-adbc-f3f9cbba5fa2"}:
            .: {}
            f:apiVersion: {}
            f:blockOwnerDeletion: {}
            f:controller: {}
            f:kind: {}
            f:name: {}
            f:uid: {}
      f:spec:
        f:containers:
          k:{"name":"spark-worker"}:
            .: {}
            f:env:
              .: {}
              k:{"name":"BASH_DEBUG"}:
                .: {}
                f:name: {}
                f:value: {}
              k:{"name":"SPARK_DAEMON_JAVA_OPTS"}:
                .: {}
                f:name: {}
              k:{"name":"SPARK_DAEMON_MEMORY"}:
                .: {}
                f:name: {}
              k:{"name":"SPARK_MASTER_URL"}:
                .: {}
                f:name: {}
                f:value: {}
              k:{"name":"SPARK_MODE"}:
                .: {}
                f:name: {}
                f:value: {}
              k:{"name":"SPARK_WORKER_OPTS"}:
                .: {}
                f:name: {}
              k:{"name":"SPARK_WORKER_WEBUI_PORT"}:
                .: {}
                f:name: {}
                f:value: {}
            f:image: {}
            f:imagePullPolicy: {}
            f:livenessProbe:
              .: {}
              f:failureThreshold: {}
              f:httpGet:
                .: {}
                f:path: {}
                f:port: {}
                f:scheme: {}
              f:initialDelaySeconds: {}
              f:periodSeconds: {}
              f:successThreshold: {}
              f:timeoutSeconds: {}
            f:name: {}
            f:ports:
              .: {}
              k:{"containerPort":8081,"protocol":"TCP"}:
                .: {}
                f:containerPort: {}
                f:name: {}
                f:protocol: {}
            f:readinessProbe:
              .: {}
              f:failureThreshold: {}
              f:httpGet:
                .: {}
                f:path: {}
                f:port: {}
                f:scheme: {}
              f:initialDelaySeconds: {}
              f:periodSeconds: {}
              f:successThreshold: {}
              f:timeoutSeconds: {}
            f:resources:
              .: {}
              f:limits:
                .: {}
                f:cpu: {}
                f:memory: {}
              f:requests:
                .: {}
                f:cpu: {}
                f:memory: {}
            f:terminationMessagePath: {}
            f:terminationMessagePolicy: {}
            f:volumeMounts:
              .: {}
              k:{"mountPath":"/opt/bitnami/spark/spark_data"}:
                .: {}
                f:mountPath: {}
                f:name: {}
        f:dnsPolicy: {}
        f:enableServiceLinks: {}
        f:hostname: {}
        f:restartPolicy: {}
        f:schedulerName: {}
        f:securityContext:
          .: {}
          f:fsGroup: {}
          f:runAsUser: {}
        f:subdomain: {}
        f:terminationGracePeriodSeconds: {}
        f:volumes:
          .: {}
          k:{"name":"spark-data"}:
            .: {}
            f:name: {}
            f:persistentVolumeClaim:
              .: {}
              f:claimName: {}
    manager: kube-controller-manager
    operation: Update
    time: "2020-12-14T16:14:55Z"
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:status:
        f:conditions:
          k:{"type":"ContainersReady"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
          k:{"type":"Initialized"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
          k:{"type":"Ready"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
        f:containerStatuses: {}
        f:hostIP: {}
        f:phase: {}
        f:podIP: {}
        f:podIPs:
          .: {}
          k:{"ip":"10.34.0.1"}:
            .: {}
            f:ip: {}
        f:startTime: {}
    manager: kubelet
    operation: Update
    time: "2020-12-16T10:47:54Z"
  name: my-release-spark-worker-0
  namespace: default
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: StatefulSet
    name: my-release-spark-worker
    uid: 505ce7bc-d2c7-4646-adbc-f3f9cbba5fa2
  resourceVersion: "10202027"
  selfLink: /api/v1/namespaces/default/pods/my-release-spark-worker-0
  uid: d220d559-b7a3-4a9d-b771-2c3cc8929893
spec:
  containers:
  - env:
    - name: SPARK_MODE
      value: worker
    - name: BASH_DEBUG
      value: "0"
    - name: SPARK_DAEMON_MEMORY
    - name: SPARK_WORKER_WEBUI_PORT
      value: "8081"
    - name: SPARK_DAEMON_JAVA_OPTS
    - name: SPARK_MASTER_URL
      value: spark://my-release-spark-master-svc:7077
    - name: SPARK_WORKER_OPTS
    image: docker.io/bitnami/spark:2.4.6-debian-10-r14
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 6
      httpGet:
        path: /
        port: 8081
        scheme: HTTP
      initialDelaySeconds: 180
      periodSeconds: 20
      successThreshold: 1
      timeoutSeconds: 5
    name: spark-worker
    ports:
    - containerPort: 8081
      name: http
      protocol: TCP
    readinessProbe:
      failureThreshold: 6
      httpGet:
        path: /
        port: 8081
        scheme: HTTP
      initialDelaySeconds: 30
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 5
    resources:
      limits:
        cpu: "2"
        memory: 2Gi
      requests:
        cpu: "1"
        memory: 1Gi
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /opt/bitnami/spark/spark_data
      name: spark-data
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-glpld
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  hostname: my-release-spark-worker-0
  nodeName: kubeadm-k1
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext:
    fsGroup: 0
    runAsUser: 0
  serviceAccount: default
  serviceAccountName: default
  subdomain: my-release-spark-headless
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: spark-data
    persistentVolumeClaim:
      claimName: spark-data-my-release-spark-worker-0
  - name: default-token-glpld
    secret:
      defaultMode: 420
      secretName: default-token-glpld
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2020-12-14T16:14:55Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2020-12-16T10:45:21Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2020-12-16T10:45:21Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2020-12-14T16:14:55Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: docker://f5b5b2bff4b982b7df355c10aaa0234da03b3bf8fc4a5bf4cdecef50b0112afc
    image: bitnami/spark:2.4.6-debian-10-r14
    imageID: docker-pullable://bitnami/spark@sha256:466bf61dd153c004d08fccf6f4e3ae9f560664736e4b867230d613d343d767e4
    lastState:
      terminated:
        containerID: docker://c45398ce26dbf566f115ecb5f79f05e89a7a9b37f102bd68abe3c83b743aa9d2
        exitCode: 1
        finishedAt: "2020-12-16T10:44:43Z"
        reason: Error
        startedAt: "2020-12-16T10:34:31Z"
    name: spark-worker
    ready: true
    restartCount: 217
    started: true
    state:
      running:
        startedAt: "2020-12-16T10:44:44Z"
  hostIP: 172.17.13.106
  phase: Running
  podIP: 10.34.0.1
  podIPs:
  - ip: 10.34.0.1
  qosClass: Burstable
  startTime: "2020-12-14T16:14:55Z"
